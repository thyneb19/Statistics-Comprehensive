<!--
You can delete the header-includes (lines 3-5 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap2.Rmd will compile by itself when you hit Knit PDF.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(dplyr)
library(lubridate)
library(ggplot2)
library(randomForest)
library(iml)
library(caret)
library(gridExtra)
library(knitr)

if(!require(acstats)){
  library(devtools)
  devtools::install_github("Amherst-Statistics/acstats")
  }
```

#Applying Shapley Values

##King's County Housing Data

To demonstrate the IML package and how shapley values work for larger datasets, I will use shapley values to better understand the multiples models I have trained to predict King's County Washington housing prices. The data can be found here https://www.kaggle.com/shivachandel/kc-house-data, and the variables in the dataset are as follows:

* Date: Date of the home sale
* Price: price of the home sold
* Bedrooms: number of bedrooms
* Bathrooms: number of bathrooms (0.5 is a bathroom with a toilet and no shower)
* Sqft_living: square footage of the apartment’s interior living space
* Sqft_lot: square footage of the land space
* Floors: number of floors
* Waterfront: whether an apartment is overlooking the waterfront or not
* View: an index from 0 to 4 indicating how good the view from the property was
* Condition: an index from 1 to 5 indicating how good condition the property was in
* Grade: index from 1 to 13 where 1-3 falls short of building construction and design, 7 has an average level of construction and design, 11-13 has a high level of construction and design
* Sqft_above: the square footage of the interior space that is above ground level
* Sqft_basement: the square footage of the interior space that is below ground level
* Yr_built: the year the house was build
* Yr_renovated: the year of the house’s last renovation
* Zipcode: zipcode of the house
* Lat: latitude of the house
* Long: longitude of the house
* Sqft_living15: the square footage of interior living space for the nearest 15 neighbors
* Sqft_lot15: the square footage of land lots for the nearest 15 neighbors

```{r cleanHouse, include = FALSE}
#read in housing data
data = read.csv("data/kc_house_data.csv")
data$date = ymd_hms(data$date)
data = data %>% mutate(year = year(date))
data = data %>% mutate(age = year-yr_built)

#convert appropriate variables into factors
data$year = as.factor(data$year)
data$yr_built = as.factor(data$yr_built)
data$floors = as.factor(data$floors)
data$view = as.factor(data$view)
data$view = as.factor(data$view)
data$condition = as.factor(data$condition)
data$grade = as.factor(data$grade)
data$waterfront = as.factor(data$waterfront)

#create categorical variable to tell if a house's price is greater than $650000 and if the house has been renovated
data = data %>% mutate(priceCat = ifelse(price>650000, 1, 0), renovated = ifelse(yr_renovated == 0, 0, 1))
data$renovated = as.factor(data$renovated)
data$priceCat = as.factor(data$priceCat)
```

```{r timeVPricePlots, echo = FALSE, results = "asis"}
label(path = "figure/timeVPrice.png", 
      caption = "Relationships between age of home and year sold with price the home was sold at", 
      label = "timeVarRelationships", type = "figure",
      scale = 0.4)
```

```{r catVPricePlots, echo = FALSE, results = "asis"}
label(path = "figure/categoricalVPrice.PNG", 
      caption = "Distribution of house price across different levels of waterfront, view, condition, grade, and renovated", 
      label = "catVarRelation", type = "figure",
      scale = 0.4)
```

```{r roomsFloorsVPricePlots, echo = FALSE, results = "asis"}
label(path = "figure/roomsAndFloorsVPrice.PNG", 
      caption = "Distribution of house price across homes with different number of bedrooms, bathrooms, and floors", 
      label = "roomAndFloorVPrice", type = "figure",
      scale = 0.4)
```

```{r sqftSpaceVPricePlots, echo = FALSE, results = "asis"}
label(path = "figure/sqftVPrice.PNG", 
      caption = "Distribution of house price across different levels of waterfront, view, condition, grade, and renovated", 
      label = "square footage vs price", type = "figure",
      scale = 0.4)
```

In figure 2.1 we can look at the the distribution of house price across year sold (all homes were sold in either 2014 or 2015) and the relationship between age of a home and its house price. We can see that the distribution of house prices across 2014 and 2015 is quite similar, and when we take a look at the how age relates to house price we do not see any significant linear relationship. Furthermore, when we look at the distribution of house ages for homes sold for more that $\$650000$ and homes sold for less than or equal to $\$650000$, the distributions are also appear quite similar. This would lead us to believe that age of a home may not be a strong indicator of the home's price.

In figure 2.2 we can look at the distribution of house price across homes with and without waterfront views, different levels of view quality, different conditions and grades, and homes that have been renovated or not. We can see homes with a waterfront view, on average, are sold at higher prices than homes without a waterfront view. Furthermore, although we see that homes with view qualities of 1, 2, and 3 have a similar distribution of house price, homes with the highest quality of views (4) appear to be sold for significantly higher prices.
Across different levels of condition, homes with the best condition (5) have the highest average house price compared to the other condition levels. However due to the large amount of spread in each boxplot, there is a lot of overlap between these distributions across the condition levels. This leads us to believe that house condition may not be a strong predictor for house price. We also see a lot of overlap between the distribution of house price for unrenovated and renovated homes. While renovated homes on average fetch a higher price, there is not as big of a difference between the distributions as initially expected. Grade appears to be a stronger predictor for house price. Homes with higher grades have a much higher average price sold than homes with lower grades. 

In figure 2.3 we can see how the number of bedrooms, bathrooms, and floors of a home relate to its price. In all three graphs, there does not appear to be a strong relationship. There may be a very slight positive relationship between the number of bedrooms or bathrooms with house price, but these variables do not appear to be strong predictors of price. Looking at the distributions of house prices across homes with different numbers of floors, we find something unexpected. Homes with 2.5 floors appear to cost the most on average and cost significantly more than homes with 1, 3, and 3.5 floors. We would expect homes with more floors to be bigger and thus tend to cost more, but this does not appear to always be the case. 

In figure 2.4 we look at how the square footage of different parts of a home relates to the home's price. Across all different measures of square footage, sqft_living, sqft_living15, and sqft_above, we see a slight positive linear relationship. For sqft_lot, sqft_lot15, and sqft_basement, we see a peak followed by a downward curve. Besides these strange relationships, we generally see that homes with larger living spaces fetch higher prices.

```{r makeLatLongGraph, include = FALSE}
latLongplot <- ggplot(data, aes(x = long, y = lat, col = priceCat)) + geom_density_2d() + ggtitle("latitude and longitude vs price category")
```
```{r housingDens_plot, echo = FALSE, results = "asis"}
label(path = "figure/latlogplot.png", 
      caption = "Density of expensive and less expensive houses by lat and long", 
      label = "house locations", type = "figure",
      scale = 1)
```

From the 2-dimentional density plot in figure 2.5, we can see that the more expensive houses (with price $>\$650000$) tend to be above latitude 47.5. Less expensive houses (price $\leq\$650000$) a more widely distributed. We can use this information to create a new predictor variable latThresh.

```{r createLatThresh, include = FALSE}
#create new variable out of latitude bound observed in the data
data = data %>% mutate(latThresh = ifelse(lat>=47.5,1,0))
data$latThresh = as.factor(data$latThresh)
#ggplot(data, aes(x = latThresh, y = price)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1000000)) + ggtitle("latThresh vs house price")
```

##Creating our Models

The first thing to notice when creating models to predict house price is the fact that there are much fewer expensive houses (price $>\$650000$). 

```{r}
summary(data$priceCat)
```

Because of this, when creating the training data we will sample $\frac{2}{3}$ of the observations from each price category. After creating out training and test sets, we will use the caret package to tune our randomforest models.

For the housing data I am primarily interested in predicting two things, price of the house sold and whether or not the house was sold for $\$650000$ or more. To do this I created two randomforest models, one that predicted the continuous variable price and one to predict the categorical variable priceCat (1 if house price $> \$650000$, else 0). 

```{r readInModels, include = FALSE}
#models are quite large, read in models from files instead of recalculating them
rfCont = readRDS("models/rfPrice.rds")
rfCat = readRDS("models/rfPriceCat.rds")
```
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
set.seed(1337)
dataCleanedCat = data[,-match(c("id", "date", "zipcode", "yr_built", "yr_renovated", "lat", "long"), colnames(data))]
dataCleanedCat$year = as.factor(dataCleanedCat$year)
lowerPriceData = dataCleanedCat %>% dplyr::filter(priceCat == 0)
higherPriceData = dataCleanedCat %>% dplyr::filter(priceCat == 1)

#dealing with very unbalanced dataset, have to make sure to get enough of the less represented category in the trainingset
trainLowerIndices = sample(nrow(lowerPriceData), 0.66*nrow(lowerPriceData))
trainHigherIndices = sample(nrow(higherPriceData), 0.66*nrow(higherPriceData))

trainLower = lowerPriceData[trainLowerIndices,]
trainHigher = higherPriceData[trainHigherIndices,]
testLower = lowerPriceData[-trainLowerIndices,]
testHigher = higherPriceData[trainHigherIndices,]

train = rbind(trainLower, trainHigher)
trainCat = train[,-match("price", colnames(train))]
trainCont = train[,-match("priceCat", colnames(train))]
test = rbind(testLower, testHigher)
testCat = test[,-match("price", colnames(test))]
testCont = test[,-match("priceCat", colnames(test))]
```

```{r eval = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#create own RF function so we can optimize mtry and ntree
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

control <- trainControl(method="repeatedcv", number=10, repeats=3, search = "grid")
tunegrid = expand.grid(.mtry = c(1:10), .ntree = c(450, 500, 550, 600))

rfCat = train(priceCat ~., data = trainCat, method = customRF, tuneGrid = tunegrid, trControl = control)
rf = train(price ~., data = trainCat, method = customRF, tuneGrid = tunegrid, trControl = control)
```

For both of the models, I used the caret package to determine optimal hyperparameters for the model. The optimal mtry parameters found for each model was 5 for the predicting price, and 4 for predicting priceCat. Both models have an nTree value of 500. The MSE for the continuous predictor model is 11672358367 and the accuracy for the categorical model is $97.58\%$.

```{r}
#somewhere waterfront was converted into a numeric had to fix this
testCont$waterfront = as.factor(testCont$waterfront)
testCat$waterfront = as.factor(testCat$waterfront)
testCont$floors = as.factor(testCont$floors)
testCat$floors = as.factor(testCat$floors)

rfCont$mtry
rfCat$mtry
mean((predict(rfCont, testCont)-testCont$price)^2)
table(predict(rfCat, testCat), testCat$priceCat)
mean(predict(rfCat, testCat)==testCat$priceCat)
```

##Using IML Shapley Values

It is important to reiterate the interpretation of shapley values once again. For some feature value $j$, $\phi_j$ represents their shapley value. The feature value $j$ contributed $\phi_j$ to the prediction of its observation compared to the average prediction across the dataset. 

The figure 2.6 shows  the shapley values (labelled phi on the x-axis) for each feature of an observation in the housing dataset. At the top of the plot we can see the actual prediction our randomforest model made for this observation, as well as the average of all predictions made on the entire housing dataset. On the y-axis we can see each predictor variable as well as their corresponding value for this observation. This observation is a home that has a latThresh value of 1, age of 59 years, 1 floor, sqftlot15 of 5650 square feet, sqftlot of 5650, no waterfront view, no renovations, lowest quality of view, 3 bedrooms, sold in the year 2014, condition of 3, sqftbasement of 0 square feet (no basement), 1 bathroom, sqftabove of 1180 square feet, sqftliving15 of 1340 square feet, grade of 7, and sqftliving of 1180 square feet. 

What the phi value tells us is that, all feature values with phi value less than 0 had a negative contribution and lowered the predicted price;conversely, all phi values greater than 0 had a positive contribution and increased the predicted price.[@molnar_2019] For example, the home's sqftliving value of 1180 square feet decreased the predicted price by $\$85550$ compared to the average predicted price of $\$540968.59$. Looking at the distribution of sqftliving values across the housing dataset, this home's sqftliving of 1180 is below the 1st quartile of all home's sqftliving, which is 1427 square feet. Thus this home would be considered a smaller home and this intuitively would have us believe that this would cause the home to cost less.  The shapley value here captures this intuition. We see that the sum of all the shapley values describes the total difference between the prediction and average prediction, $\sum_{j\subseteq features}\phi_j = 248172.43-540968.59$.

```{r shap1_plot, echo = FALSE, results = "asis"}
label(path = "figure/quantShapExample", 
      caption = "Feature Value Contributions to Predicted House Price for A Single Observation", 
      label = "shapPrice", type = "figure",
      scale = 0.4)
```

In figure 2.7 we can see how shapley values work for models predicting categorical variables. We now have 2 graphs showing the probability that an observation is either in category 0, $price \leq \$650000$, or category 1, $price >\$650000$.Instead of contributions to a numerical prediction, feature values contribute to a probability of being within a certain category. We see that most of the example observation's feature values contribute a positive phi value for category 0 and negative phi value for category 1. Each of these feature values then increase the probability of the observation being in category 0 and decrease the probability of the observation being in category 1. Again, the sqftliving for this home is 1180 square feet which is relatively small. We then see that this feature value decreases the probability that the home costs more than $\$650000$ and increases the probability that the home costs less than or equal to $\$650000$.

```{r shap2_plot, echo = FALSE, results = "asis"}
label(path = "figure/catShapExample", 
      caption = "Feature Value Contributions to Predicted House Price Category (>$650000 or <= $650000) for A Single Observation", 
      label = "shapCatPrice", type = "figure",
      scale = 0.4)
```

After calculating shapley values for a large sample of observations in the housing dataset, we can get a better idea of the contributions of each feature to the prediction of housing price. In the tables 2.1. and 2.2 we can see that the predictor variables with the greatest contributions (average of the absolute value of their shapley values) to house price are latThresh, grade, sqftliving, and age. The predictor variables with the greatest contributions to predicted price category ($>\$650000 or \leq\$650000$) are grade, latThresh, sqftliving, and age. Although it is somewhat surprising to see age as one of the most contributing predictor variables, since there did not appear to be a strong relationship between age and house price in our data exploration, the other top 3 predictor variables match our initial findings as well as our intuition about the dataset. Home size, location, and grade (construction quality) are all expected to greatly influence the price of a home. 

\begin{table}[ht]
\centering
\begin{tabular}{rlr}
  \hline
 & feature & aveAbsShap \\ 
  \hline
1 & latThresh & 84006.10 \\ 
  2 & grade & 73075.63 \\ 
  3 & sqft\_living & 61451.27 \\ 
  4 & age & 38550.82 \\ 
  5 & sqft\_living15 & 33025.58 \\ 
  6 & sqft\_above & 24302.07 \\ 
  7 & condition & 11439.19 \\ 
  8 & sqft\_lot15 & 11139.27 \\ 
  9 & view & 9860.34 \\ 
  10 & bathrooms & 8621.50 \\ 
  11 & sqft\_basement & 7057.55 \\ 
  12 & sqft\_lot & 6476.98 \\ 
  13 & year & 3623.03 \\ 
  14 & floors & 2923.12 \\ 
  15 & renovated & 2292.26 \\ 
  16 & waterfront & 2068.81 \\ 
  17 & bedrooms & 1741.82 \\ 
   \hline
\end{tabular}
\caption{Mean Absolute Contribution of Predictors for Model Predicting Price} 
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rlr}
  \hline
 & feature & aveAbsShap \\ 
  \hline
1 & grade & 0.05 \\ 
  2 & latThresh & 0.04 \\ 
  3 & sqft\_living & 0.03 \\ 
  4 & age & 0.03 \\ 
  5 & sqft\_living15 & 0.02 \\ 
  6 & sqft\_above & 0.02 \\ 
  7 & sqft\_basement & 0.01 \\ 
  8 & sqft\_lot15 & 0.01 \\ 
  9 & condition & 0.01 \\ 
  10 & sqft\_lot & 0.01 \\ 
  11 & bathrooms & 0.01 \\ 
  12 & view & 0.01 \\ 
  13 & renovated & 0.01 \\ 
  14 & floors & 0.01 \\ 
  15 & bedrooms & 0.01 \\ 
  16 & year & 0.00 \\ 
  17 & waterfront & 0.00 \\ 
   \hline
\end{tabular}
\caption{Mean Absolute Contribution of Predictors for Model Predicting Whether or not a House is worth more than 650000 Dollars} 
\end{table}

```{r, echo = FALSE, results = "asis"}
#varImpPlot(rfCont, main = "Importance of Predictors for Model Predicting House Price")
#varImpPlot(rfCat, main = "Importance of Predictors for Model Predicting Whether a House is worth more than $650000 or not")


label(path = "figure/catVarImp", caption = "Categorical Prediction Model Variable Importance", 
      label = "priceCatVarImportance", type = "figure",
      scale = 0.4)

label(path = "figure/quantVarImp", caption = "Quantitative Prediction Model Variable Importance", 
      label = "priceQuantVarImportance", type = "figure",
      scale = 0.4)
```

When we take a look at the variable importance of the randomforest model in figure 2.8, we can see that sqft_living, latThresh, and grade are also among the top most important variables. The similarity between the most important variables according to Gini index and Shapley Values shows us that feature contribution can be a good measurement of variable importance if calculated for the entire dataset. There are in fact methods that perform feature selection using shapley values, but such methods go beyond the scope of this report. 

Shapley values also let us explore what kind of feature values contribute to higher or lower house prices. For instance, when we separate sqft_living values by whether they have a positive or negative shapley value, we can get an idea of what sized home costs more.

```{r, echo = FALSE, results = "asis"}
label(path = "figure/sqftlivingPosNegShap.PNG", caption = "comparison of sqftliving values with negative vs positive shapley values (blue mean positive contribution, red means negative contribution)", label = "sqft_living across positive and negative shapley values", type = "figure", scale = 0.4)
```

In figure 2.10 we see that the distribution for sqft_living values with negative shapley values is centered around 1500 square feet while the distribution for sqft_living values with positive shapley values achieves its peak around 2750 square feet. The two distributions have a bit of overlap, but in general we see that homes with sqftliving larger than 2000 square feet will fetch a higher price than homes with sqftliving less than 2000 square feet. Again, this observation from shapley values matches the basic intuition that larger homes are typically more expensive than smaller homes.

```{r, echo = FALSE, results = "asis"}
label(path = "figure/viewContr", caption = "comparison of view levels with negative vs positive shapley values", label = "view levels across positive and negative shapley values", type = "figure", scale = 1)
```

We can also affirm our intuition about the prices of homes with better views. In figure 2.11 we see that the only view level that decreased an observation's prediction compared to the average prediction were views of level 0. However when we look at all the view values with a positive contribution, we find that view levels 1 through 4 increased the predicted home price compared to the average predicted price. This would suggest that having a view increases the price of a home, again, supporting basic intuition.