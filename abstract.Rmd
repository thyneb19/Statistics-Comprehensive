The use of automated machine learning models has become more and more ubiquitous in many industries today. However, while these methods may be great for prediction, the complexity nature of their training make it difficult to understand the models and interpretation of their results even more so. In an effort to improve model interpretability for individual observations, this report will discuss the calculation of shapley values. Shapley values are a measure of what an observation’s feature value contributes to a model’s prediction and can be incredibly useful due to the relative ease of their interpretation. To fully understand shapley values, this report will walkthrough how to calculate and approximate them using the IML R package. Included is also a study of the King’s County housing dataset using shapley values, which demonstrates how these measurements of feature contribution can confirm human intuition about data.

